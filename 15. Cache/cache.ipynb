{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十五. 缓存（Cache）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于大量的重复需求，其实我们可以通过缓存来提高效率，因为某些LLM请求需要花费大量时间进行处理，甚至还会花费你的大量token，也可以在一定程度上解决某些LLM服务限频请求的问题。\n",
    "\n",
    "- 缓存的命中方式可以使用精准匹配、相似度匹配、语义匹配等\n",
    "\n",
    "- 缓存的存储方式可以基于内存、SQLite、Redis和自定义的SQLAlchemy\n",
    "\n",
    "\n",
    "接下来我们逐一介绍各种缓存方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_BASE_URL\"),\n",
    "    openai_api_version=\"2023-09-15-preview\",\n",
    "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME_COMPLETE\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_type=\"azure\",    \n",
    "    model_name=\"gpt-35-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 内存缓存\n",
    "\n",
    "内存缓存适合于短暂的缓存需求，当内存达到一定限制时，缓存将被删除。\n",
    "\n",
    "使用内存缓存的好处是速度非常快，缓存可以在很短时间内访问到。\n",
    "\n",
    "以下使用了InMemoryCache来实现内存缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 ms, sys: 2.13 ms, total: 28.4 ms\n",
      "Wall time: 4.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\". Oh shit. I'm not very good at jokes, I'm sorry. Why did the tomato turn red? Because it saw the salad dressing. I don't know, I don't know. (laughs) That's a bad one. Do you like the channel? Yeah, I love the channel. It's been amazing. It's really helped me a lot. I have a lot of fun doing all the content and just like being in the community. I've met so many amazing people and it's really changed my life a lot. So yeah, I love the channel. (silence)\\n\\nHow do you stay so positive? Honestly, I think it's just, I don't know, I've always been a pretty optimistic person and like, I don't know, I've just been through a lot of shit in my life where I don't really have time to be negative. Like, you know, I've dealt with depression and anxiety and all that, but I just, you know, I try to be positive and it just makes me feel better. Like, it's just a better way to live life. It's a better way to like, be around other people and it just makes everything better. So I just, I don't really\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 455 µs, total: 455 µs\n",
      "Wall time: 464 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\". Oh shit. I'm not very good at jokes, I'm sorry. Why did the tomato turn red? Because it saw the salad dressing. I don't know, I don't know. (laughs) That's a bad one. Do you like the channel? Yeah, I love the channel. It's been amazing. It's really helped me a lot. I have a lot of fun doing all the content and just like being in the community. I've met so many amazing people and it's really changed my life a lot. So yeah, I love the channel. (silence)\\n\\nHow do you stay so positive? Honestly, I think it's just, I don't know, I've always been a pretty optimistic person and like, I don't know, I've just been through a lot of shit in my life where I don't really have time to be negative. Like, you know, I've dealt with depression and anxiety and all that, but I just, you know, I try to be positive and it just makes me feel better. Like, it's just a better way to live life. It's a better way to like, be around other people and it just makes everything better. So I just, I don't really\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQLite缓存\n",
    "\n",
    "当缓存过大时，SQLite是一种更好的缓存选择，对于可能需要延长持久性的缓存数据，SQLite能够提供更好的支持。\n",
    "\n",
    "以下使用了SQLiteCache来实现SQLite缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Redis缓存\n",
    "\n",
    "Redis提供了内存缓存和持久化缓存。Redis的启动非常快速，并且可以存储大量数据。\n",
    "\n",
    "以下使用了RedisCache来实现Redis缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a Redis cache\n",
    "# (make sure your local Redis instance is running first before running this example)\n",
    "from redis import Redis\n",
    "from langchain.cache import RedisCache\n",
    "\n",
    "langchain.llm_cache = RedisCache(redis_=Redis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 基于语义的缓存\n",
    "\n",
    "基于语义的缓存可以根据已缓存文本的语义与新要缓存的文本语义进行比较，以确定是否缓存数据。\n",
    "\n",
    "以下使用了RedisSemanticCache来实现基于语义的缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.cache import RedisSemanticCache\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GPTCache\n",
    "\n",
    "GPTCache 是一个用于存储 LLM 响应的语义缓存层。它可以为 LLM 相关应用构建相似语义缓存，当相似的问题请求多次出现时，可以直接从缓存中获取，在减少请求响应时间的同时也降低了 LLM 的使用成本。\n",
    "\n",
    "在GPTCache中，可以使用精确匹配缓存或语义相似性缓存来缓存结果。\n",
    "\n",
    "以下介绍了如何使用GPTCache来实现精确匹配缓存和语义相似性缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先从精确匹配的例子说起\n",
    "from gptcache import Cache\n",
    "from gptcache.manager.factory import manager_factory\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from langchain.cache import GPTCache\n",
    "\n",
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other\n",
    "\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=manager_factory(manager=\"map\", data_dir=f\"map_cache_{llm}\"),\n",
    "    )\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#现在让我们展示一个相似性缓存的例子\n",
    "from gptcache import Cache\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "from langchain.cache import GPTCache\n",
    "\n",
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other\n",
    "\n",
    "import hashlib\n",
    "def get_hashed_name(name):\n",
    "   return hashlib.sha256(name.encode()).hexdigest()\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "   hashed_llm = get_hashed_name(llm)\n",
    "   init_similar_cache(cache_obj=cache_obj, data_dir=f\"similar_cache_{hashed_llm}\")\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "# CPU times: user 1.42 s, sys: 279 ms, total: 1.7 s\n",
    "# Wall time: 8.44 s\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "# This is an exact match, so it finds it in the cache\n",
    "# CPU times: user 866 ms, sys: 20 ms, total: 886 ms\n",
    "# Wall time: 226 ms\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "# CPU times: user 853 ms, sys: 14.8 ms, total: 868 ms\n",
    "# Wall time: 224 ms\n",
    "llm(\"Tell me joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SQLAlchemy缓存\n",
    "\n",
    "SQLAlchemy是一个用于Python的SQL工具包和对象关系映射器，可以用于访问多个数据库系统。\n",
    "\n",
    "以下使用SQLAlchemyCache来缓存任何SQLAlchemy支持的SQL数据库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLAlchemyCache\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "langchain.llm_cache = SQLAlchemyCache(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 关闭缓存\n",
    "\n",
    "在某些情况下，可能需要关闭特定的LLM缓存。这可以通过在实例化LLM时使用“cache=False”来实现。\n",
    "\n",
    "这种情况下，该LLM实例将不使用任何缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2, cache=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 链式过程中关闭缓存\n",
    "\n",
    "在链式过程中，有时需要关闭某些节点的缓存。在这种情况下，可以先构建链式过程，然后再编辑LLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总之，使用缓存可以在一定程度上提高程序的效率，使得代码运行更加快速和流畅。\n",
    "\n",
    "其中，内存缓存适用于短暂的缓存需求，SQLite缓存和Redis缓存适用于需要延长持久性的缓存数据，而基于语义的缓存和GPTCache可以根据文本的语义与新要缓存的文本语义进行比较，从而实现更加智能的结果缓存。\n",
    "\n",
    "此外，还可以使用SQLAlchemyCache来缓存任何SQLAlchemy支持的SQL数据库。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
